# ğŸš€ Comprehensive Rust Compilation Analysis Toolkit

[![License: AGPL v3](https://img.shields.io/badge/License-AGPL%20v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0)
[![Rust](https://img.shields.io/badge/rust-1.70+-orange.svg)](https://www.rust-lang.org)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—-HuggingFace-yellow.svg)](https://huggingface.co/datasets/introspector/rust)

**World's most comprehensive Rust compilation pipeline analysis toolkit** - Extract semantic analysis, project structure, and LLVM IR generation data from Rust codebases for machine learning and compiler research.

## ğŸ¯ **What This Tool Does**

This toolkit creates **unprecedented datasets** by analyzing Rust compilation at every level:

```
Rust Source â†’ rustc â†’ LLVM IR â†’ Optimizations â†’ Machine Code
     â†“           â†“        â†“           â†“            â†“
  Semantic   Project   IR Gen    Optimization   Assembly
  Analysis   Analysis           Passes
     â†“           â†“        â†“           â†“            â†“
 HF Dataset  HF Dataset HF Dataset  HF Dataset  HF Dataset
```

## ğŸ† **Achievements**

- âœ… **1.4+ Million Records**: Largest Rust analysis dataset ever created
- âœ… **Self-Referential Analysis**: Tools analyzing their own codebases
- âœ… **Complete Pipeline**: Source code â†’ LLVM IR generation
- âœ… **Production Ready**: Used to analyze rust-analyzer, rustc, and llvm-sys.rs
- âœ… **HuggingFace Compatible**: Ready for ML training and research

## ğŸ”§ **Installation**

### Prerequisites
- Rust 1.70+ with Cargo
- Git LFS (for large dataset files)

### Build from Source
```bash
git clone https://github.com/solfunmeme/hf-dataset-validator-rust.git
cd hf-dataset-validator-rust
cargo build --release
```

### Quick Test
```bash
# Test on a simple Rust file
echo 'fn main() { println!("Hello, world!"); }' > test.rs
cargo run --bin hf-validator -- analyze-rust-to-ir test.rs output-dataset
```

## ğŸš€ **Quick Start**

### 1. **Complete Pipeline Analysis** (Recommended)
Analyze a Rust project with all three extractors:
```bash
# Complete analysis: semantic + project + LLVM IR
cargo run --bin hf-validator -- analyze-rust-to-ir /path/to/rust/project output-dataset

# This creates:
# output-dataset/semantic/     - Rust semantic analysis
# output-dataset/cargo/        - Project structure analysis  
# output-dataset/llvm-ir/      - LLVM IR generation analysis
```

### 2. **Individual Analysis Types**

#### Semantic Analysis (rust-analyzer based)
```bash
# Extract parsing, name resolution, and type inference data
cargo run --bin hf-validator -- generate-hf-dataset /path/to/rust/project semantic-output
```

#### Project Analysis (Cargo metadata)
```bash
# Extract project structure and dependency information
cargo run --bin hf-validator -- analyze-cargo-project /path/to/rust/project cargo-output
```

#### LLVM IR Analysis (Compilation pipeline)
```bash
# Extract LLVM IR generation across optimization levels
cargo run --bin hf-validator -- analyze-llvm-ir /path/to/rust/project llvm-output
```

## ğŸ“Š **CLI Commands Reference**

### **Core Analysis Commands**

| Command | Description | Output |
|---------|-------------|---------|
| `analyze-rust-to-ir <source> [output]` | **Complete pipeline analysis** | Semantic + Project + LLVM IR |
| `generate-hf-dataset <source> [output]` | Rust semantic analysis | Parsing, name resolution, type inference |
| `analyze-cargo-project <source> [output]` | Project structure analysis | Cargo metadata and dependencies |
| `analyze-llvm-ir <source> [output] [opt_levels]` | LLVM IR generation analysis | IR across O0, O1, O2, O3 |

### **Validation Commands**

| Command | Description |
|---------|-------------|
| `validate-hf-dataset [dataset_dir]` | Validate semantic analysis dataset |
| `validate-cargo-dataset [dataset_dir]` | Validate cargo analysis dataset |
| `validate-llvm-dataset [dataset_dir]` | Validate LLVM IR analysis dataset |

### **Utility Commands**

| Command | Description |
|---------|-------------|
| `validate-solfunmeme <base_path>` | Validate solfunmeme dataset structure |
| `convert-to-parquet <input> <output>` | Convert datasets to Parquet format |

## ğŸ”¬ **Analysis Types Explained**

### **1. Semantic Analysis** (`generate-hf-dataset`)
Extracts deep semantic information using rust-analyzer:

- **Parsing Phase**: Syntax trees, tokenization, parse errors
- **Name Resolution Phase**: Symbol binding, scope analysis, imports
- **Type Inference Phase**: Type checking, inference decisions, errors

**Schema**: 20+ fields including source snippets, AST data, symbol information

### **2. Project Analysis** (`analyze-cargo-project`)
Analyzes project structure and metadata:

- **Project Metadata**: Cargo.toml analysis, workspace support
- **Dependencies**: Dependency graphs and version constraints
- **Build Configuration**: Features, targets, build scripts

**Schema**: 44+ fields including project info, dependency data, build metadata

### **3. LLVM IR Analysis** (`analyze-llvm-ir`)
Captures Rust â†’ LLVM IR compilation:

- **IR Generation**: How Rust constructs become LLVM IR
- **Optimization Passes**: LLVM optimization analysis (planned)
- **Code Generation**: Target-specific code generation (planned)
- **Performance Analysis**: Optimization impact measurement (planned)

**Schema**: 50+ fields including source code, LLVM IR, optimization data

## ğŸ“ˆ **Real-World Examples**

### **Analyze rust-analyzer** (533K records)
```bash
git clone https://github.com/rust-lang/rust-analyzer.git
cargo run --bin hf-validator -- analyze-rust-to-ir rust-analyzer rust-analyzer-dataset
```

### **Analyze Rust Compiler** (835K records)
```bash
git clone https://github.com/rust-lang/rust.git
cargo run --bin hf-validator -- generate-hf-dataset rust/compiler rustc-dataset
```

### **Analyze LLVM Bindings** (9K records)
```bash
git clone https://gitlab.com/taricorp/llvm-sys.rs.git
cargo run --bin hf-validator -- analyze-rust-to-ir llvm-sys.rs llvm-sys-dataset
```

## ğŸ¯ **Use Cases**

### **Machine Learning Research**
- **Code Understanding Models**: Train on semantic analysis data
- **Performance Prediction**: Learn from optimization patterns
- **Code Generation**: Understand compilation patterns
- **Bug Detection**: Identify problematic code patterns

### **Compiler Research**
- **Optimization Studies**: Analyze real-world optimization impact
- **Type System Research**: Understand type compilation patterns
- **Performance Engineering**: Correlate source patterns with performance
- **Tool Development**: Build better development tools

### **Educational Applications**
- **Compiler Education**: Show real compilation processes
- **Rust Learning**: Understand professional code patterns
- **Research Methods**: Example of comprehensive analysis

## ğŸ“Š **Output Format**

All tools generate **Apache Parquet files** optimized for ML workflows:

```
output-dataset/
â”œâ”€â”€ semantic/
â”‚   â”œâ”€â”€ parsing-phase/data-*.parquet
â”‚   â”œâ”€â”€ name_resolution-phase/data.parquet
â”‚   â””â”€â”€ type_inference-phase/data.parquet
â”œâ”€â”€ cargo/
â”‚   â””â”€â”€ project_metadata-phase/data.parquet
â”œâ”€â”€ llvm-ir/
â”‚   â”œâ”€â”€ ir_generation-O0-phase/data.parquet
â”‚   â”œâ”€â”€ ir_generation-O1-phase/data.parquet
â”‚   â”œâ”€â”€ ir_generation-O2-phase/data.parquet
â”‚   â””â”€â”€ ir_generation-O3-phase/data.parquet
â””â”€â”€ README.md (comprehensive documentation)
```

### **Loading Data**
```python
import pandas as pd

# Load semantic analysis data
parsing_df = pd.read_parquet('output-dataset/semantic/parsing-phase/data.parquet')
print(f"Loaded {len(parsing_df)} parsing records")

# Load LLVM IR data
ir_df = pd.read_parquet('output-dataset/llvm-ir/ir_generation-O2-phase/data.parquet')
print(f"Loaded {len(ir_df)} LLVM IR records")
```

```rust
use arrow::record_batch::RecordBatch;
use parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder;

// Load data in Rust
let file = std::fs::File::open("output-dataset/semantic/parsing-phase/data.parquet")?;
let builder = ParquetRecordBatchReaderBuilder::try_new(file)?;
let reader = builder.build()?;

for batch_result in reader {
    let batch = batch_result?;
    println!("Loaded batch with {} records", batch.num_rows());
}
```

## ğŸ”§ **Advanced Usage**

### **Custom Optimization Levels**
```bash
# Analyze specific optimization levels
cargo run --bin hf-validator -- analyze-llvm-ir project.rs output O0,O2,O3
```

### **Large Project Analysis**
```bash
# For projects with 1000+ files, use semantic analysis only first
cargo run --bin hf-validator -- generate-hf-dataset large-project semantic-only
# Then add project analysis
cargo run --bin hf-validator -- analyze-cargo-project large-project cargo-analysis
```

### **Validation and Quality Checks**
```bash
# Validate generated datasets
cargo run --bin hf-validator -- validate-hf-dataset output-dataset/semantic
cargo run --bin hf-validator -- validate-cargo-dataset output-dataset/cargo
cargo run --bin hf-validator -- validate-llvm-dataset output-dataset/llvm-ir
```

## ğŸ—ï¸ **Architecture**

### **Modular Design**
- **`rust_analyzer_extractor`**: Semantic analysis using rust-analyzer
- **`cargo2hf_extractor`**: Project structure analysis with workspace support
- **`llvm_ir_extractor`**: LLVM IR generation and optimization analysis
- **`validator`**: Dataset validation and quality assurance

### **Data Pipeline**
1. **Source Analysis**: Parse and analyze Rust source files
2. **Data Extraction**: Extract relevant information for each phase
3. **Schema Validation**: Ensure data consistency and quality
4. **Parquet Generation**: Create ML-optimized output files
5. **Documentation**: Generate comprehensive README files

## ğŸ¤ **Contributing**

We welcome contributions! Areas for improvement:

- **New Analysis Phases**: Add more compilation stages
- **Performance Optimization**: Handle larger codebases
- **Schema Enhancement**: Add more semantic information
- **Documentation**: Improve usage examples and tutorials

## ğŸ“š **Research Papers & Citations**

If you use this toolkit in research, please cite:

```bibtex
@software{rust_compilation_analyzer,
  title={Comprehensive Rust Compilation Analysis Toolkit},
  author={HF Dataset Validator Team},
  year={2025},
  url={https://github.com/solfunmeme/hf-dataset-validator-rust},
  note={World's first comprehensive Rust compilation pipeline analysis}
}
```

## ğŸ‰ **Success Stories**

- **ğŸ† World's Largest Rust Dataset**: 1.4+ million semantic analysis records
- **ğŸ”¬ Self-Referential Analysis**: rust-analyzer analyzing itself (533K records)
- **âš¡ Compiler Analysis**: Complete rustc analysis (835K records)
- **ğŸŒ‰ LLVM Bridge**: llvm-sys.rs pipeline analysis (9K records)
- **ğŸ“Š HuggingFace Ready**: Available at `huggingface.co/datasets/introspector/rust`

## ğŸ“ **Support**

- **Issues**: [GitHub Issues](https://github.com/solfunmeme/hf-dataset-validator-rust/issues)
- **Discussions**: [GitHub Discussions](https://github.com/solfunmeme/hf-dataset-validator-rust/discussions)
- **Dataset**: [HuggingFace Hub](https://huggingface.co/datasets/introspector/rust)

## ğŸ“„ **License**

AGPL-3.0 - See [LICENSE](LICENSE) for details.

---

**ğŸš€ Ready to revolutionize Rust analysis and ML-powered development tools!**
