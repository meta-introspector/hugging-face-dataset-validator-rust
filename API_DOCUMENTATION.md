# 🔧 API Documentation

This document provides comprehensive API documentation for the **Comprehensive Rust Compilation Analysis Toolkit**, including schemas, data formats, and programmatic access patterns.

## 📊 **Dataset Schemas**

### **1. Semantic Analysis Schema** (`RustAnalyzerRecord`)

Generated by `generate-hf-dataset` and the semantic component of `analyze-rust-to-ir`.

#### **Core Fields**
| Field | Type | Description |
|-------|------|-------------|
| `id` | `string` | Unique identifier for the analysis event |
| `file_path` | `string` | Path to the source file being analyzed |
| `line` | `uint32` | Line number in source file |
| `column` | `uint32` | Column number in source file |
| `phase` | `string` | Processing phase (`parsing`, `name_resolution`, `type_inference`) |
| `element_type` | `string` | Type of code element (`function`, `struct`, `variable`, etc.) |
| `element_name` | `string` | Name of the element (if applicable) |

#### **Analysis Data Fields**
| Field | Type | Description |
|-------|------|-------------|
| `syntax_data` | `string` (JSON) | Serialized syntax tree information |
| `symbol_data` | `string` (JSON) | Serialized symbol resolution data |
| `type_data` | `string` (JSON) | Serialized type inference information |
| `source_snippet` | `string` | The actual source code being analyzed |
| `context_before` | `string` | Source code context before the element |
| `context_after` | `string` | Source code context after the element |

#### **Metadata Fields**
| Field | Type | Description |
|-------|------|-------------|
| `processing_time_ms` | `uint64` | Time taken for analysis (milliseconds) |
| `processing_order` | `uint32` | Order of processing for reproducibility |
| `timestamp` | `uint64` | Unix timestamp when record was created |
| `rust_version` | `string` | Version of Rust compiler used |
| `analyzer_version` | `string` | Version of rust-analyzer used |
| `extractor_version` | `string` | Version of extraction tool |

#### **Example Record**
```json
{
  "id": "main.rs:1:1:parsing:function:main",
  "file_path": "/path/to/main.rs",
  "line": 1,
  "column": 1,
  "phase": "parsing",
  "element_type": "function",
  "element_name": "main",
  "syntax_data": "{\"kind\":\"FN\",\"text_range\":\"0..42\"}",
  "symbol_data": null,
  "type_data": null,
  "source_snippet": "fn main() { println!(\"Hello!\"); }",
  "context_before": "",
  "context_after": "",
  "processing_time_ms": 1,
  "processing_order": 1,
  "timestamp": 1704672000,
  "rust_version": "1.86.0",
  "analyzer_version": "0.3.1",
  "extractor_version": "0.1.0"
}
```

### **2. Project Analysis Schema** (`CargoProjectRecord`)

Generated by `analyze-cargo-project` and the cargo component of `analyze-rust-to-ir`.

#### **Project Identity Fields**
| Field | Type | Description |
|-------|------|-------------|
| `id` | `string` | Unique identifier for the project record |
| `project_path` | `string` | Path to the project root |
| `project_name` | `string` | Name from Cargo.toml |
| `project_version` | `string` | Version from Cargo.toml |
| `phase` | `string` | Analysis phase (`project_metadata`, `dependency_analysis`, etc.) |

#### **Project Metadata Fields**
| Field | Type | Description |
|-------|------|-------------|
| `description` | `string?` | Project description |
| `authors` | `string?` (JSON array) | Project authors |
| `license` | `string?` | License identifier |
| `repository` | `string?` | Repository URL |
| `homepage` | `string?` | Homepage URL |
| `documentation` | `string?` | Documentation URL |
| `keywords` | `string?` (JSON array) | Keywords for discoverability |
| `categories` | `string?` (JSON array) | Crate categories |

#### **Code Metrics Fields**
| Field | Type | Description |
|-------|------|-------------|
| `lines_of_code` | `uint32` | Total lines of code |
| `source_file_count` | `uint32` | Number of source files |
| `test_file_count` | `uint32` | Number of test files |
| `example_file_count` | `uint32` | Number of example files |
| `benchmark_file_count` | `uint32` | Number of benchmark files |
| `complexity_score` | `float32` | Code complexity metric |
| `documentation_coverage` | `float32` | Documentation coverage percentage |

#### **Dependency Fields**
| Field | Type | Description |
|-------|------|-------------|
| `direct_dependencies` | `uint32` | Number of direct dependencies |
| `total_dependencies` | `uint32` | Total dependency count (including transitive) |
| `dev_dependencies` | `uint32` | Number of dev dependencies |
| `build_dependencies` | `uint32` | Number of build dependencies |
| `dependency_data` | `string?` (JSON) | Detailed dependency information |
| `features` | `string?` (JSON) | Available features |
| `targets` | `string?` (JSON) | Build targets |

#### **Build Configuration Fields**
| Field | Type | Description |
|-------|------|-------------|
| `has_build_script` | `bool` | Whether project has build.rs |
| `build_script_complexity` | `uint32` | Build script complexity score |

#### **Ecosystem Fields**
| Field | Type | Description |
|-------|------|-------------|
| `download_count` | `uint64?` | Crates.io download count |
| `github_stars` | `uint32?` | GitHub star count |
| `github_forks` | `uint32?` | GitHub fork count |
| `github_issues` | `uint32?` | Open GitHub issues |
| `last_updated` | `uint64?` | Last update timestamp |
| `commit_count` | `uint32?` | Git commit count |
| `contributor_count` | `uint32?` | Number of contributors |
| `project_age_days` | `uint32?` | Project age in days |
| `release_frequency` | `float32?` | Average releases per month |

### **3. LLVM IR Analysis Schema** (`LLVMIRRecord`)

Generated by `analyze-llvm-ir` and the LLVM IR component of `analyze-rust-to-ir`.

#### **Source Context Fields**
| Field | Type | Description |
|-------|------|-------------|
| `id` | `string` | Unique identifier for the IR record |
| `source_file` | `string` | Path to the source Rust file |
| `construct_name` | `string` | Function or construct name |
| `phase` | `string` | Analysis phase (`ir_generation`, `optimization_passes`, etc.) |
| `rust_source` | `string` | Original Rust source code |
| `source_line` | `uint32` | Line number in source |
| `source_column` | `uint32` | Column number in source |
| `rust_construct_type` | `string` | Rust construct type |
| `rust_type_info` | `string?` | Rust type information |

#### **LLVM IR Fields**
| Field | Type | Description |
|-------|------|-------------|
| `llvm_ir` | `string` | Generated LLVM IR code |
| `ir_instruction_count` | `uint32` | Number of LLVM IR instructions |
| `ir_basic_block_count` | `uint32` | Number of basic blocks |
| `llvm_function_signature` | `string?` | LLVM function signature |
| `llvm_type_mappings` | `string?` (JSON) | Type mappings |

#### **Optimization Fields**
| Field | Type | Description |
|-------|------|-------------|
| `optimization_passes` | `string?` (JSON array) | Applied optimization passes |
| `ir_before_optimization` | `string?` | IR before optimization |
| `ir_after_optimization` | `string?` | IR after optimization |
| `optimization_impact_score` | `float32` | Optimization impact metric |
| `performance_improvement` | `float32` | Performance improvement estimate |

#### **Code Generation Fields**
| Field | Type | Description |
|-------|------|-------------|
| `target_architecture` | `string` | Target architecture (e.g., "x86_64") |
| `assembly_code` | `string?` | Generated assembly code |
| `assembly_instruction_count` | `uint32` | Assembly instruction count |
| `register_usage` | `string?` (JSON) | Register usage analysis |
| `memory_patterns` | `string?` (JSON) | Memory usage patterns |

#### **Performance Fields**
| Field | Type | Description |
|-------|------|-------------|
| `estimated_cycles` | `uint64?` | Estimated execution cycles |
| `code_size_bytes` | `uint32` | Code size in bytes |
| `complexity_score` | `float32` | Code complexity metric |
| `optimization_level` | `string` | Optimization level (O0, O1, O2, O3) |

#### **Type System Fields**
| Field | Type | Description |
|-------|------|-------------|
| `type_mapping_analysis` | `string?` (JSON) | Rust → LLVM type mappings |
| `generic_handling` | `string?` | Generic parameter handling |
| `trait_object_info` | `string?` | Trait object representation |
| `lifetime_analysis` | `string?` | Lifetime analysis impact |

#### **Memory Analysis Fields**
| Field | Type | Description |
|-------|------|-------------|
| `stack_allocations` | `string?` (JSON array) | Stack allocation patterns |
| `heap_allocations` | `string?` (JSON array) | Heap allocation patterns |
| `memory_safety_preserved` | `bool` | Memory safety guarantees preserved |
| `reference_counting` | `string?` | Reference counting usage |

## 🔌 **Programmatic Access**

### **Python API**

#### **Loading Datasets**
```python
import pandas as pd
import pyarrow.parquet as pq
from pathlib import Path

class RustAnalysisDataset:
    def __init__(self, dataset_path: str):
        self.dataset_path = Path(dataset_path)
        
    def load_semantic_data(self, phase: str = "parsing") -> pd.DataFrame:
        """Load semantic analysis data for a specific phase."""
        phase_dir = self.dataset_path / "semantic" / f"{phase}-phase"
        
        # Handle multiple files
        parquet_files = list(phase_dir.glob("*.parquet"))
        if not parquet_files:
            raise FileNotFoundError(f"No parquet files found in {phase_dir}")
            
        # Load all files and concatenate
        dfs = [pd.read_parquet(f) for f in parquet_files]
        return pd.concat(dfs, ignore_index=True)
    
    def load_cargo_data(self) -> pd.DataFrame:
        """Load project analysis data."""
        cargo_file = self.dataset_path / "cargo" / "project_metadata-phase" / "data.parquet"
        return pd.read_parquet(cargo_file)
    
    def load_llvm_ir_data(self, optimization_level: str = "O2") -> pd.DataFrame:
        """Load LLVM IR analysis data for specific optimization level."""
        ir_file = self.dataset_path / "llvm-ir" / f"ir_generation-{optimization_level}-phase" / "data.parquet"
        return pd.read_parquet(ir_file)
    
    def get_dataset_summary(self) -> dict:
        """Get summary statistics for the dataset."""
        summary = {}
        
        # Semantic data summary
        for phase in ["parsing", "name_resolution", "type_inference"]:
            try:
                df = self.load_semantic_data(phase)
                summary[f"semantic_{phase}"] = {
                    "records": len(df),
                    "files": df["file_path"].nunique(),
                    "elements": df["element_type"].value_counts().to_dict()
                }
            except FileNotFoundError:
                summary[f"semantic_{phase}"] = {"records": 0}
        
        # Cargo data summary
        try:
            cargo_df = self.load_cargo_data()
            summary["cargo"] = {
                "projects": len(cargo_df),
                "total_dependencies": cargo_df["total_dependencies"].sum(),
                "lines_of_code": cargo_df["lines_of_code"].sum()
            }
        except FileNotFoundError:
            summary["cargo"] = {"projects": 0}
        
        # LLVM IR summary
        for opt_level in ["O0", "O1", "O2", "O3"]:
            try:
                ir_df = self.load_llvm_ir_data(opt_level)
                summary[f"llvm_ir_{opt_level}"] = {
                    "records": len(ir_df),
                    "avg_instructions": ir_df["ir_instruction_count"].mean(),
                    "avg_complexity": ir_df["complexity_score"].mean()
                }
            except FileNotFoundError:
                summary[f"llvm_ir_{opt_level}"] = {"records": 0}
        
        return summary

# Usage example
dataset = RustAnalysisDataset("rust-analyzer-complete")

# Load parsing data
parsing_df = dataset.load_semantic_data("parsing")
print(f"Loaded {len(parsing_df)} parsing records")

# Load project metadata
cargo_df = dataset.load_cargo_data()
print(f"Project: {cargo_df['project_name'].iloc[0]}")

# Load LLVM IR data
ir_df = dataset.load_llvm_ir_data("O2")
print(f"LLVM IR records: {len(ir_df)}")

# Get summary
summary = dataset.get_dataset_summary()
print("Dataset summary:", summary)
```

#### **Data Analysis Examples**
```python
# Analyze element type distribution
parsing_df = dataset.load_semantic_data("parsing")
element_counts = parsing_df["element_type"].value_counts()
print("Most common elements:", element_counts.head())

# Analyze file complexity
file_complexity = parsing_df.groupby("file_path").size().sort_values(ascending=False)
print("Most complex files:", file_complexity.head())

# Compare optimization levels
o0_df = dataset.load_llvm_ir_data("O0")
o2_df = dataset.load_llvm_ir_data("O2")

print(f"O0 avg instructions: {o0_df['ir_instruction_count'].mean():.2f}")
print(f"O2 avg instructions: {o2_df['ir_instruction_count'].mean():.2f}")
```

### **Rust API**

#### **Loading Datasets**
```rust
use arrow::record_batch::RecordBatch;
use parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder;
use std::fs::File;
use std::path::Path;
use anyhow::Result;

pub struct RustAnalysisDataset {
    dataset_path: std::path::PathBuf,
}

impl RustAnalysisDataset {
    pub fn new(dataset_path: impl AsRef<Path>) -> Self {
        Self {
            dataset_path: dataset_path.as_ref().to_path_buf(),
        }
    }
    
    pub fn load_semantic_data(&self, phase: &str) -> Result<Vec<RecordBatch>> {
        let phase_dir = self.dataset_path.join("semantic").join(format!("{}-phase", phase));
        let mut batches = Vec::new();
        
        for entry in std::fs::read_dir(phase_dir)? {
            let entry = entry?;
            let path = entry.path();
            
            if path.extension().and_then(|s| s.to_str()) == Some("parquet") {
                let file = File::open(&path)?;
                let builder = ParquetRecordBatchReaderBuilder::try_new(file)?;
                let reader = builder.build()?;
                
                for batch_result in reader {
                    batches.push(batch_result?);
                }
            }
        }
        
        Ok(batches)
    }
    
    pub fn load_cargo_data(&self) -> Result<Vec<RecordBatch>> {
        let cargo_file = self.dataset_path
            .join("cargo")
            .join("project_metadata-phase")
            .join("data.parquet");
            
        let file = File::open(cargo_file)?;
        let builder = ParquetRecordBatchReaderBuilder::try_new(file)?;
        let reader = builder.build()?;
        
        let mut batches = Vec::new();
        for batch_result in reader {
            batches.push(batch_result?);
        }
        
        Ok(batches)
    }
    
    pub fn load_llvm_ir_data(&self, optimization_level: &str) -> Result<Vec<RecordBatch>> {
        let ir_file = self.dataset_path
            .join("llvm-ir")
            .join(format!("ir_generation-{}-phase", optimization_level))
            .join("data.parquet");
            
        let file = File::open(ir_file)?;
        let builder = ParquetRecordBatchReaderBuilder::try_new(file)?;
        let reader = builder.build()?;
        
        let mut batches = Vec::new();
        for batch_result in reader {
            batches.push(batch_result?);
        }
        
        Ok(batches)
    }
    
    pub fn get_record_count(&self, phase: &str) -> Result<usize> {
        let batches = self.load_semantic_data(phase)?;
        Ok(batches.iter().map(|b| b.num_rows()).sum())
    }
}

// Usage example
fn main() -> Result<()> {
    let dataset = RustAnalysisDataset::new("rust-analyzer-complete");
    
    // Load parsing data
    let parsing_batches = dataset.load_semantic_data("parsing")?;
    println!("Loaded {} parsing batches", parsing_batches.len());
    
    // Get record count
    let parsing_count = dataset.get_record_count("parsing")?;
    println!("Total parsing records: {}", parsing_count);
    
    // Load LLVM IR data
    let ir_batches = dataset.load_llvm_ir_data("O2")?;
    println!("Loaded {} LLVM IR batches", ir_batches.len());
    
    Ok(())
}
```

### **HuggingFace Datasets Integration**

#### **Loading from HuggingFace Hub**
```python
from datasets import load_dataset

# Load the complete dataset
dataset = load_dataset("introspector/rust")

# Load specific configurations
semantic_dataset = load_dataset("introspector/rust", "semantic")
cargo_dataset = load_dataset("introspector/rust", "cargo")
llvm_dataset = load_dataset("introspector/rust", "llvm_ir")

# Access specific splits
parsing_data = semantic_dataset["parsing"]
name_resolution_data = semantic_dataset["name_resolution"]
type_inference_data = semantic_dataset["type_inference"]

print(f"Parsing records: {len(parsing_data)}")
print(f"Name resolution records: {len(name_resolution_data)}")
print(f"Type inference records: {len(type_inference_data)}")
```

#### **Streaming Large Datasets**
```python
from datasets import load_dataset

# Stream data for large datasets
dataset = load_dataset("introspector/rust", "semantic", streaming=True)

# Process in batches
for batch in dataset["parsing"].iter(batch_size=1000):
    # Process batch
    print(f"Processing batch with {len(batch['id'])} records")
    # Your ML training code here
```

## 🔍 **Query Patterns**

### **Common Analysis Queries**

#### **Find Functions by Complexity**
```python
# Load parsing data
parsing_df = dataset.load_semantic_data("parsing")

# Filter for functions
functions = parsing_df[parsing_df["element_type"] == "function"]

# Sort by source snippet length as complexity proxy
complex_functions = functions.assign(
    complexity=functions["source_snippet"].str.len()
).sort_values("complexity", ascending=False)

print("Most complex functions:")
print(complex_functions[["element_name", "file_path", "complexity"]].head())
```

#### **Analyze Type Inference Patterns**
```python
# Load type inference data
type_df = dataset.load_semantic_data("type_inference")

# Parse type data JSON
import json
type_df["parsed_type_data"] = type_df["type_data"].apply(
    lambda x: json.loads(x) if x else {}
)

# Analyze type patterns
type_patterns = type_df["parsed_type_data"].apply(
    lambda x: x.get("inferred_type", "unknown")
).value_counts()

print("Most common inferred types:")
print(type_patterns.head())
```

#### **Compare Optimization Impact**
```python
# Load different optimization levels
o0_df = dataset.load_llvm_ir_data("O0")
o2_df = dataset.load_llvm_ir_data("O2")

# Merge on construct name for comparison
comparison = o0_df.merge(
    o2_df, 
    on="construct_name", 
    suffixes=("_O0", "_O2")
)

# Calculate optimization impact
comparison["instruction_reduction"] = (
    comparison["ir_instruction_count_O0"] - comparison["ir_instruction_count_O2"]
) / comparison["ir_instruction_count_O0"]

print("Functions with highest optimization impact:")
print(comparison.nlargest(10, "instruction_reduction")[
    ["construct_name", "instruction_reduction"]
])
```

## 📈 **Performance Considerations**

### **Memory Usage**
- **Large datasets**: Use streaming or batch processing
- **Parquet files**: Efficient columnar storage, but can be memory-intensive
- **JSON fields**: Parse only when needed to save memory

### **I/O Optimization**
- **SSD storage**: Significantly faster for large datasets
- **Parallel loading**: Load multiple files concurrently
- **Selective loading**: Load only required columns

### **Processing Tips**
- **Filter early**: Apply filters before expensive operations
- **Use indices**: Create indices for frequently queried columns
- **Batch processing**: Process data in chunks for large datasets

---

**🚀 Ready to build amazing tools with our comprehensive Rust analysis data!**

For more information:
- **Documentation**: https://github.com/solfunmeme/hf-dataset-validator-rust
- **Dataset**: https://huggingface.co/datasets/introspector/rust
- **Issues**: https://github.com/solfunmeme/hf-dataset-validator-rust/issues
